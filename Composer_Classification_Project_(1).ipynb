{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamikshKodgire/AAI-511Group6_USD/blob/main/Composer_Classification_Project_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e961826d",
      "metadata": {
        "id": "e961826d"
      },
      "source": [
        "#  Composer Classification Using Deep Learning\n",
        "This notebook demonstrates a complete pipeline for classifying musical compositions by composer using deep learning. We'll use LSTM and CNN models on MIDI file data from famous composers like Mozart and Chopin."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ea47497",
      "metadata": {
        "id": "1ea47497"
      },
      "source": [
        "## üìå Methodology Overview\n",
        "1. **Data Collection**: MIDI files of compositions from various composers.\n",
        "2. **Data Pre-processing**: Convert scores to usable formats and apply augmentation.\n",
        "3. **Feature Extraction**: Extract musical features (notes, tempo, etc.).\n",
        "4. **Model Building**: Use LSTM and CNN for classification.\n",
        "5. **Model Training**\n",
        "6. **Model Evaluation**\n",
        "7. **Model Optimization**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "762c2f23",
      "metadata": {
        "id": "762c2f23"
      },
      "source": [
        "## üìÇ Step 1: Load and Explore Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f04a08d5",
      "metadata": {
        "id": "f04a08d5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "dataset_path = '/mnt/data/Composer_Dataset/Composer_Dataset/NN_midi_files_extended/test'\n",
        "for composer in os.listdir(dataset_path):\n",
        "    composer_path = os.path.join(dataset_path, composer)\n",
        "    if os.path.isdir(composer_path):\n",
        "        files = os.listdir(composer_path)\n",
        "        print(f\"{composer}: {len(files)} MIDI files\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9faa0c15",
      "metadata": {
        "id": "9faa0c15"
      },
      "source": [
        "## üéº Step 2: Preprocessing and Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bf8d0f8",
      "metadata": {
        "id": "6bf8d0f8"
      },
      "outputs": [],
      "source": [
        "import pretty_midi\n",
        "import numpy as np\n",
        "\n",
        "def extract_features(midi_path):\n",
        "    try:\n",
        "        midi_data = pretty_midi.PrettyMIDI(midi_path)\n",
        "        notes = []\n",
        "        for instrument in midi_data.instruments:\n",
        "            if not instrument.is_drum:\n",
        "                notes += [note.pitch for note in instrument.notes]\n",
        "        return np.array(notes)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {midi_path}: {e}\")\n",
        "        return np.array([])\n",
        "\n",
        "# Example usage:\n",
        "mozart_path = os.path.join(dataset_path, 'mozart')\n",
        "sample_file = os.path.join(mozart_path, os.listdir(mozart_path)[0])\n",
        "features = extract_features(sample_file)\n",
        "print(f\"Extracted {len(features)} notes from {os.path.basename(sample_file)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d36aaca3",
      "metadata": {
        "id": "d36aaca3"
      },
      "source": [
        "## üß† Step 3: Model Building (LSTM / CNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14b2c10a",
      "metadata": {
        "id": "14b2c10a"
      },
      "outputs": [],
      "source": [
        "# This section will define and compile the deep learning models.\n",
        "# TODO: Define LSTM and CNN architectures using TensorFlow/Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "765f55bc",
      "metadata": {
        "id": "765f55bc"
      },
      "source": [
        "### üß† LSTM Model for Composer Classification\n",
        "We use an LSTM model to learn the sequence of notes in compositions, assuming a fixed input length per sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6c00532",
      "metadata": {
        "id": "f6c00532"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Dummy data for demonstration (replace with actual sequences and labels)\n",
        "X = [np.random.randint(60, 72, size=100) for _ in range(200)]  # 200 samples of 100-note sequences\n",
        "y = np.random.randint(0, 2, size=(200,))  # Binary labels (e.g., Mozart vs Chopin)\n",
        "\n",
        "# Padding sequences to the same length\n",
        "X = pad_sequences(X, maxlen=100)\n",
        "\n",
        "# Define LSTM model\n",
        "model_lstm = Sequential([\n",
        "    Embedding(input_dim=128, output_dim=64, input_length=100),\n",
        "    LSTM(128, return_sequences=False),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_lstm.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d3edd48",
      "metadata": {
        "id": "2d3edd48"
      },
      "source": [
        "### üß† CNN Model for Composer Classification\n",
        "We use a CNN on a piano roll-style 2D input for each composition sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64709987",
      "metadata": {
        "id": "64709987"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Dummy piano roll input: 200 samples, 128 pitches x 100 time steps\n",
        "X_cnn = np.random.rand(200, 128, 100, 1)\n",
        "y_cnn = y\n",
        "\n",
        "# Define CNN model\n",
        "input_layer = Input(shape=(128, 100, 1))\n",
        "x = Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "output = Dense(1, activation='sigmoid')(x)\n",
        "model_cnn = Model(inputs=input_layer, outputs=output)\n",
        "\n",
        "model_cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_cnn.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49b575c5",
      "metadata": {
        "id": "49b575c5"
      },
      "source": [
        "## üéπ Note Distribution Visualization\n",
        "Before building models, it's helpful to visualize the distribution of musical notes across the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9b306b8",
      "metadata": {
        "id": "e9b306b8"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "all_notes = []\n",
        "composer_labels = []\n",
        "composer_map = {}\n",
        "label_counter = 0\n",
        "\n",
        "# Scan through each composer's folder\n",
        "for composer in os.listdir(dataset_path):\n",
        "    composer_path = os.path.join(dataset_path, composer)\n",
        "    if os.path.isdir(composer_path):\n",
        "        composer_map[label_counter] = composer\n",
        "        for midi_file in os.listdir(composer_path):\n",
        "            if not midi_file.endswith('.mid'):\n",
        "                continue\n",
        "            file_path = os.path.join(composer_path, midi_file)\n",
        "            notes = extract_features(file_path)\n",
        "            if len(notes) > 0:\n",
        "                all_notes.extend(notes)\n",
        "                composer_labels.extend([label_counter] * len(notes))\n",
        "        label_counter += 1\n",
        "\n",
        "# Plot distribution\n",
        "plt.figure(figsize=(12, 5))\n",
        "sns.histplot(x=all_notes, bins=60, kde=True)\n",
        "plt.title('Distribution of Musical Notes in Dataset')\n",
        "plt.xlabel('MIDI Note Number')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e022675",
      "metadata": {
        "id": "0e022675"
      },
      "source": [
        "## üè∑Ô∏è Generate Dataset with Composer Labels\n",
        "Here we convert each composer's name into a numeric label and build the dataset for model input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0509c86a",
      "metadata": {
        "id": "0509c86a"
      },
      "outputs": [],
      "source": [
        "# Generate labeled dataset\n",
        "X_data = []\n",
        "y_data = []\n",
        "max_seq_len = 100  # Fixed length for LSTM input\n",
        "composer_map = {}\n",
        "label_counter = 0\n",
        "\n",
        "for composer in os.listdir(dataset_path):\n",
        "    composer_path = os.path.join(dataset_path, composer)\n",
        "    if os.path.isdir(composer_path):\n",
        "        composer_map[label_counter] = composer\n",
        "        for midi_file in os.listdir(composer_path):\n",
        "            if not midi_file.endswith('.mid'):\n",
        "                continue\n",
        "            file_path = os.path.join(composer_path, midi_file)\n",
        "            notes = extract_features(file_path)\n",
        "            if len(notes) > 0:\n",
        "                padded = pad_sequences([notes], maxlen=max_seq_len)[0]\n",
        "                X_data.append(padded)\n",
        "                y_data.append(label_counter)\n",
        "        label_counter += 1\n",
        "\n",
        "# Convert to arrays\n",
        "X = np.array(X_data)\n",
        "y = np.array(y_data)\n",
        "print(f\"Loaded {len(X)} samples across {len(composer_map)} composers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2559d4d9",
      "metadata": {
        "id": "2559d4d9"
      },
      "source": [
        "## üöÇ Step 4: Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66a44c4b",
      "metadata": {
        "id": "66a44c4b"
      },
      "source": [
        "## üöÇ Step 4: Model Training and Evaluation\n",
        "We'll now train the LSTM model on real extracted features. For demonstration, we simplify training using padded note sequences.\n",
        "We'll evaluate using accuracy, confusion matrix, and classification report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b245ad4",
      "metadata": {
        "id": "6b245ad4"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model_lstm.fit(X_train, y_train, epochs=5, batch_size=16, validation_split=0.1)\n",
        "\n",
        "# Evaluate model\n",
        "y_pred = (model_lstm.predict(X_test) > 0.5).astype(int)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot(cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "383a2a93",
      "metadata": {
        "id": "383a2a93"
      },
      "source": [
        "## üîß Step 5: Hyperparameter Tuning with Keras Tuner\n",
        "We'll use Keras Tuner to search for the best number of LSTM units and learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae0f100c",
      "metadata": {
        "id": "ae0f100c"
      },
      "outputs": [],
      "source": [
        "import keras_tuner as kt\n",
        "\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=128, output_dim=hp.Choice('embedding_dim', [32, 64, 128]), input_length=100))\n",
        "    model.add(LSTM(hp.Int('lstm_units', min_value=64, max_value=256, step=64)))\n",
        "    model.add(Dropout(hp.Float('dropout', min_value=0.2, max_value=0.5, step=0.1)))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(hp.Float('lr', 1e-4, 1e-2, sampling='log')),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=5,\n",
        "    executions_per_trial=1,\n",
        "    directory='tuner_results',\n",
        "    project_name='composer_lstm'\n",
        ")\n",
        "\n",
        "tuner.search(X_train, y_train, epochs=5, validation_split=0.1, verbose=1)\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "best_model.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa0b08f2",
      "metadata": {
        "id": "aa0b08f2"
      },
      "outputs": [],
      "source": [
        "# TODO: Train the models with extracted features and labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0430908",
      "metadata": {
        "id": "b0430908"
      },
      "source": [
        "## üìä Step 5: Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e305dab",
      "metadata": {
        "id": "2e305dab"
      },
      "outputs": [],
      "source": [
        "# TODO: Evaluate models using accuracy, precision, recall, and confusion matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "872304fb",
      "metadata": {
        "id": "872304fb"
      },
      "source": [
        "## üîß Step 6: Model Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bc6e61a",
      "metadata": {
        "id": "3bc6e61a"
      },
      "outputs": [],
      "source": [
        "# TODO: Apply hyperparameter tuning (e.g., learning rate, dropout, layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6db3e74a",
      "metadata": {
        "id": "6db3e74a"
      },
      "source": [
        "## ‚úÖ Summary\n",
        "- Dataset contains classical MIDI files.\n",
        "- We extracted musical note sequences using `pretty_midi`.\n",
        "- Next steps involve model training and optimization."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}